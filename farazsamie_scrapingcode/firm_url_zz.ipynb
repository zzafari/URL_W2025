{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6767eb3f-9b71-45d7-8112-ab5f24089360",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Author: Zorah Zafari \n",
    "## Date: Jan 17, 2025\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### I am trying to scrape the web links for the payday loan firms listed on this website. There are 15 pages and the code should loop through the page and print the URLS listed on each page here.\n",
    "\n",
    "#### https://www.bbb.org/us/category/payday-loans?page=1\n",
    "\n",
    "#### The issue is that everytime you reload the page, the urls change. There are 7,000 firms that populate but about 200-ish are listed on the 15 pages and there is no way to know if one firm is going to printed twice. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008d208-51b5-43fd-8330-5f944dc1411a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def get_firm_urls(page_url):\n",
    "    \"\"\"\n",
    "    Fetches all firm URLs from a given page using Selenium to handle CAPTCHA and dynamic content.\n",
    "    \"\"\"\n",
    "    driver.get(page_url)\n",
    "    print(f\"Accessing page: {page_url}\")\n",
    "    \n",
    "    # Wait for user to solve CAPTCHA manually, if prompted (just press enter when the security thing pops up)\n",
    "    input(\"If prompted, please solve the CAPTCHA and press Enter to continue...\")\n",
    "    time.sleep(5)  # Allow some additional time for the page to fully load\n",
    "\n",
    "    # Parse the rendered page with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    firm_urls = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if '/profile/' in link['href']:  # Adjust pattern as necessary for the specific structure\n",
    "            full_url = 'https://www.bbb.org' + link['href']\n",
    "            print(f\"Found URL: {full_url}\")  # Debugging output to verify URLs\n",
    "            firm_urls.append(full_url)\n",
    "    return firm_urls\n",
    "\n",
    "# Main Scraping Loop\n",
    "base_url = 'https://www.bbb.org/us/category/payday-loans?page='\n",
    "all_firm_urls = []\n",
    "\n",
    "for page in range(1, 16):  # Loop through pages 1 to 15\n",
    "    page_url = base_url + str(page)\n",
    "    firm_urls = get_firm_urls(page_url)\n",
    "    all_firm_urls.extend(firm_urls)\n",
    "    time.sleep(2)  # Additional delay between page requests\n",
    "\n",
    "# Save URLs to a CSV file\n",
    "csv_filename = 'firm_urls.csv'\n",
    "with open(csv_filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Firm URL'])  # Header row\n",
    "    for url in all_firm_urls:\n",
    "        writer.writerow([url])\n",
    "\n",
    "print(f\"Total firm URLs collected: {len(all_firm_urls)}\")\n",
    "print(f\"URLs saved to {csv_filename}\")\n",
    "\n",
    "# Close the WebDriver when done\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
