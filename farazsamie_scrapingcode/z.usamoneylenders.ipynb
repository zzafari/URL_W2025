{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.usamoneylenders.com/payday-loans--credit/\n",
    "\n",
    "#*Not Working Yet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Extract office details \n",
    "def tm_offices(lender_url):\n",
    "\n",
    "    driver.get(lender_url)\n",
    "    print(f\"Accessing page: {lender_url}\")\n",
    "    \n",
    "    # Wait for the user to solve CAPTCHA manually, if prompted\n",
    "    input(\"Solve CAPTCHA if prompted, then press Enter to continue...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Parse the rendered page with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # office data\n",
    "    office_data = []\n",
    "\n",
    "    # Extracting office info *need to work on this section*\n",
    "    for office in soup.find_all('div', class_='office-details'):\n",
    "        office_name = office.find('h3').text.strip() if office.find('h3') else 'N/A'\n",
    "        office_address = office.find('p', class_='address').text.strip() if office.find('p', class_='address') else 'N/A'\n",
    "        office_city = office.find('p', class_='city').text.strip() if office.find('p', class_='city') else 'N/A'\n",
    "        office_zip = office.find('p', class_='zip').text.strip() if office.find('p', class_='zip') else 'N/A'\n",
    "        office_state = office.find('p', class_='state').text.strip() if office.find('p', class_='state') else 'N/A'\n",
    "        office_phone = office.find('span', class_='phone').text.strip() if office.find('span', class_='phone') else 'N/A'\n",
    "\n",
    "        \n",
    "        # Add the extracted data to the office_data list\n",
    "        office_data.append([office_name, office_address, office_city, office_zip, office_state, office_phone])\n",
    "\n",
    "    return office_data\n",
    "\n",
    "# Main scraping loop\n",
    "lender_url = \"https://www.usamoneylenders.com/payday-loans--credit/\"\n",
    "\n",
    "page_number = 1\n",
    "all_office_data = []\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page_number}...\")\n",
    "    \n",
    "    # scrape data from the current page\n",
    "    office_data = tm_offices(lender_url)\n",
    "    if not office_data:\n",
    "        # Stop if no more data\n",
    "        break  \n",
    "\n",
    "    # Add the office data to the final list\n",
    "    all_office_data.extend(office_data)\n",
    "\n",
    "    # Check if there is a \"Next\" button \n",
    "    try:\n",
    "        next_button = driver.find_element(By.ID, 'DataTables_Table_O_next')\n",
    "        # Click \"Next\" if the button is enabled\n",
    "        if next_button.is_enabled(): \n",
    "            next_button.click()\n",
    "            time.sleep(10)  \n",
    "            page_number += 1\n",
    "        else:\n",
    "            # Exit if no \"Next\" button or it's disabled\n",
    "            break  \n",
    "    except Exception as e:\n",
    "        break\n",
    "\n",
    "# Save data to a CSV file\n",
    "with open('office_data.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "     # Write header\n",
    "    writer.writerow(['Office Name', 'Address', 'Phone Number', 'City', 'state',])  \n",
    "    # Write all rows of office data\n",
    "    writer.writerows(all_office_data) \n",
    "\n",
    "print(f\"Total number of offices: {len(all_office_data)}\")\n",
    "\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
